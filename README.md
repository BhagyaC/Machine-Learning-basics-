# Machine-Learning-basics-

### Gradient descent algorithm.
**Cost function or error function** are the method of calculating how much error that causes occurs in the predictions.
Gradient descetn algorithm is nothing but a way to select the optimized variables which reducec the cost function.

so imagine the variable with a minimal value say 0. then fit it to the cost function and take the derivative(delta). Derivative are nothing but slop which gaves an intution on the direction which way the value should move inorder to optimize.

variable = variable - (alpha * delta). Alph  is the learning parameter.
